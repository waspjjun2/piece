{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import request\n",
    "from requests.compat import urljoin, urlparse\n",
    "from requests.exceptions import HTTPError\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canfetch(url, agent='*', path='/'):\n",
    "    robot = RobotFileParser(urljoin(url, '/robots.txt'))\n",
    "    robot.read()\n",
    "    return robot.can_fetch(agent, urlparse(url)[2])\n",
    "    \n",
    "def download(url, params={}, headers={}, method='GET', limit=3):\n",
    "    if canfetch(url) == False:\n",
    "        print('[Error] ' + url)\n",
    "#     else: # 실제 수집할 때, 제약사항이 많으므로 여기선 잠시 해제\n",
    "    try:\n",
    "        resp = request(method, url,\n",
    "               params=params if method=='GET' else {},\n",
    "               data=params if method=='POST' else {},\n",
    "               headers=headers)\n",
    "        resp.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        if limit > 0 and e.response.status_code >= 500:\n",
    "            print(limit)\n",
    "            time.sleep(1) # => random\n",
    "            resp = download(url, params, headers, method, limit-1)\n",
    "        else:\n",
    "            print('[{}] '.format(e.response.status_code) + url)\n",
    "            print(e.response.status_code)\n",
    "            print(e.response.reason)\n",
    "            print(e.response.headers)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.google.com/search'\n",
    "params = {\n",
    "    'q':'',\n",
    "    'oq':'',\n",
    "    'aqs':'chrome..69i57j69i59j69i65l3j69i61j69i60j69i61.1205j0j7',\n",
    "    'sourceid':'chrome',\n",
    "    'ie':'UTF-8'\n",
    "}\n",
    "params['q'] = params['oq'] = '파이썬'\n",
    "headers = {\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
    "}\n",
    "resp = download(url, params, headers, 'GET')\n",
    "# dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "# # for _ in dom.find_all('h3', {'class':'LC20lb'}):\n",
    "# #     print(_.text.strip(), _.find_parents('a')[0]['href'])\n",
    "# for _ in dom.select('div.r > a'):\n",
    "#     print(_['href'], _.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://search.naver.com/search.naver'\n",
    "params = {\n",
    "    'sm':'top_hty',\n",
    "    'fbm':0,\n",
    "    'ie':'utf8',\n",
    "    'query':''\n",
    "}\n",
    "params['query'] = '파이썬'\n",
    "resp = download(url, params, headers, 'GET')\n",
    "dom = BeautifulSoup(resp.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in dom.find_all('ul', {'class':'type01'}):\n",
    "    for a in [dt.find('a') for dt in _.find_all('dt')]:\n",
    "        print(a['href'], a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in dom.select('.type01 dt > a'):\n",
    "    print(_['href'], _.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://search.daum.net/search'\n",
    "params = {\n",
    "    'w':'tot',\n",
    "    'DA':'YZR',\n",
    "    't__nil_searchbox':'btn',\n",
    "    'sug':'',\n",
    "    'sugo':'',\n",
    "    'sq':'',\n",
    "    'o':'',\n",
    "    'q':''\n",
    "}\n",
    "params['q'] = '파 이 썬'\n",
    "resp = download(url, params, headers, 'GET')\n",
    "dom = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dom.find_all('ul', {'class':'list_info mg_cont clear'})\n",
    "for _ in dom.find_all('div', {'class':'wrap_tit'}):\n",
    "    print(_.find('a')['href'], _.find('a').text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = download('http://pythonscraping.com/pages/page3.html')\n",
    "dom = BeautifulSoup(resp.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag, #id, .class, .class.class.class\n",
    "\n",
    "ul.class, ul.class.class.class\n",
    "tag, tag, tag => CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom.select_one('div#footer') == dom.select_one('#footer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom.select_one('#footer').text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selector\n",
    "tag1, tag2\n",
    "tag1 tag2 => 자손 (find_all(recursive=True))\n",
    "tag1 > tag2 => 자식 (file_all(recursive=False))\n",
    "tag1 + tag2 => 형제(다음 노드) => tag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom.select_one('#footer').find_parent().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dom.select('#wrapper > div')), len(dom.select('#wrapper > *'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_.name for _ in dom.select('#wrapper > *')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom.select_one('h1 + div').name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom.select_one('h1 + div').find_previous_sibling().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom.select_one('body > div > h1 + div').name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_.text.strip() for _ in dom.select('.gift > td:nth-of-type(3)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_['src'].strip() for _ in dom.select('.gift > td > img')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://example.webscraping.com/places/default/index'\n",
    "resp = download(url)\n",
    "dom = BeautifulSoup(resp.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.pop() 꺼낼, urls.append() 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queue => FIFO => pop(0)\n",
    "Stack => LIFO => pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list()\n",
    "seen = list()\n",
    "urls.append(url)\n",
    "\n",
    "while urls: # Queue\n",
    "    seed = urls.pop(0) # starting url\n",
    "    seen.append(seed) # => 재방문 회피\n",
    "    dom = BeautifulSoup(download(seed).text, 'html.parser') # HTTP\n",
    "#     for _ in dom.select('a'): # extract hyperlinks\n",
    "#         if _.has_attr('href'): # 나중에\n",
    "#             if _['href'].startswith('/'): # filter1\n",
    "#                 newUrls = urljoin(seed, _['href']) # Normalization\n",
    "#                 # query부분 (GET방식에서 ? 이후에 나오는 파라미터 생략)\n",
    "#                 if newUrls not in seen and newUrls not in urls: # \n",
    "#                     urls.append(newUrls)\n",
    "# #                     print(newUrls)\n",
    "    for _ in [_['href'] for _ in dom.select('a')\n",
    "              if _.has_attr('href') and _['href'].startswith('/')]:\n",
    "        newUrls = urljoin(seed, urlparse(_)[2])\n",
    "        if newUrls not in seen and newUrls not in urls:\n",
    "            urls.append(newUrls)\n",
    "    print(len(urls), len(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http => ht{2}p\n",
    "https => https? => http, https\n",
    "helllllllllo => hel{1,}o\n",
    "다.                  그래서 => _+ => _\n",
    "다. 그래서\n",
    "[http]{3,4} => [h]tttp, [t]ttp, [p]pp => h | t | p\n",
    "ttph, http, htp\n",
    "h[t ]{1,}p\n",
    "65:[A-Z] => [ABCDEFGHIZKLMNoPQR... Z] 대소문자 구별\n",
    "97:[a-z] => [...]\n",
    "[ㄱ-ㅎ] => 자음\n",
    "[ㅏ-ㅣ] => 모음\n",
    "[가-힣] => 음절\n",
    "([가-힣]+)\n",
    "^x => SQL(x%)\n",
    "x$ => SQL(%x)\n",
    "[^x] => not x\n",
    "\\b => 공백 + 문자 + 공백\n",
    "\\B => 문자 + ? + 문자\n",
    "\\b[A-Za-z]+\\b => File Edit View Insert\n",
    "\\B[A-Za-z]+\\B => F(il)e E(di)t V(ie)w I(nser)t\n",
    "a href=\"javascript:~();#\" => \\W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '''\n",
    "park 800905-1049118\n",
    "kim  700905-1059119\n",
    "'''\n",
    "pattern = re.compile('(\\d{6})[-]\\d{7}')\n",
    "pattern.search(data)\n",
    "print(pattern.sub('\\g<1>-*******', data))\n",
    "# data[6:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(r'Crow|Servo', 'HelloServoCrowHello')\n",
    "re.search(r'^Crow|^Servo', 'HelloServoCrowHello')\n",
    "re.search(r'Hello$', 'HelloServoCrowHello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'(ABC)+', 'ABCABCA A').group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'\\bclass\\b', 'no class at all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'\\Bclass\\b', 'one subclass is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'\\Bclass\\B', 'the declassified algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.search(r'(\\w+) (\\w+)', 'Issac Newton, physicist')\n",
    "result.groups(), \\\n",
    "result.group(0), result.group(1), result.group(2)\n",
    "# $@#%단어 단어\n",
    "# \\b(\\w+)\\b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'(\\w+) (\\w+)', r'\\g<2> \\g<1>', 'Issac Newton, physicist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "010-0000-0000 => 핸드폰번호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '010-0001-0002'\n",
    "data = '010 0000 0000'\n",
    "data = '010.0000.0000'\n",
    "data = '01000010002'\n",
    "data = '0232901338'\n",
    "result = re.search(r'(\\d{1,3})[- .]?(\\d{1,4})[- .]?(\\d{1,4})', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.group(0), result.group(1), result.group(2), result.group(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'\\d+[-]\\d+[-]\\d+', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = '+82 2-0001-0002'\n",
    "# data = '+82 10-0001-0002'\n",
    "data = '+82 010-0001-0002'\n",
    "# data = '82 010-0001-0002'\n",
    "# ==> ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'[+]?\\d+.?\\d+.?\\d+.?\\d+', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = download('http://pythonscraping.com/pages/page3.html')\n",
    "# dom = BeautifulSoup(resp.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.search(r'([$]\\d+[.]\\d+)', resp.text).groups()\n",
    "re.findall(r'[$]\\d+[.]\\d+', resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'[$]\\d+[.]\\d+', resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'../img/gifts/img1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in re.findall(r'src=\"(.+?img\\d[.]jpg)\"', resp.text):\n",
    "    print(urljoin(resp.request.url, _))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'<span class=\"excitingNote\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'<(span) class=\"[^\"]+\">(.+)</\\1>', resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_.strip() for _ in dom.find_all(text=re.compile(r'[.]\\d+$'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "이메일\n",
    "아이디@도메인\n",
    "회사명.com|net|org|kr\n",
    "    .co.kr|or.kr|go.kr|ac.kr\n",
    "\n",
    "아이디 규칙:영어로시작, 특수문자X, -, _, 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '''\n",
    "    test1@test.com\n",
    "    test2@test.co.kr\n",
    "    test3@email.test.com\n",
    "    test4@email.test.co.kr\n",
    "    12test@test.com\n",
    "    한글@test.com\n",
    "    test@test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\b[A-Za-z][A-Za-z0-9-_.]+@[A-Za-z0-9-_]+(?:[.][A-Za-z0-9-_]+)+',\n",
    "           data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\w+@', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://도메인 => O\n",
    "https://도메인 => O\n",
    "    \n",
    "ftp://도메인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '''\n",
    "    http://domain.com\n",
    "    http://www.domain.com\n",
    "    http://www.domain.co.kr\n",
    "    https://www.domain.com\n",
    "    https://www.domain.co.kr\n",
    "    https://www.domain.co.kr/\n",
    "    https://domain.co.kr/service\n",
    "    https://www.domain.co.kr/service\n",
    "    https://www.domain.co.kr/service/\n",
    "    https://domain.co.kr/service?key=value\n",
    "    https://www.domain.co.kr/service?key=value\n",
    "    https://www.domain.co.kr/service?key=value/\n",
    "    https://www.domain.co.kr/service/1/2?key=value\n",
    "    https://www.domain.co.kr/service/1/2?key=value/\n",
    "    ?key=value\n",
    "    www.domain.com?key=value\n",
    "    http://old.mail.domain.com?key=value\n",
    "    http://domain.com#key=value\n",
    "    http://www.domain.com?key=value\n",
    "    http://www.domain.com#key=value\n",
    "'''\n",
    "# Query String (?이후부분) 빼고, URI(L)을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'https?://\\w+(?:[./]\\w+)+', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\b파이썬\\B(\\w+)\\b', resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\b(\\w+)\\B파이썬\\b', resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\b파이썬\\b\n",
    "\\b(\\w+)\\B파이썬\\b\n",
    "\\b파이썬\\B(\\w+)\\b\n",
    "\n",
    "파이썬 파이썬 파이썬 = 3\n",
    "\n",
    "~파이썬 파이썬~\n",
    "은, 는, 이, 가~ => 명사\n",
    "강이지를, 강이지가, 강아지는 => Unique\n",
    "강아지+?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "()(강아지)(를) => \n",
    "()(강아지)(가) =>\n",
    "()(강아지)(는) =>\n",
    "(강아지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.google.com/search'\n",
    "params = {\n",
    "    'q':'',\n",
    "    'oq':'',\n",
    "    'aqs':'chrome..69i57j69i59j69i65l3j69i61j69i60j69i61.1205j0j7',\n",
    "    'sourceid':'chrome',\n",
    "    'ie':'UTF-8'\n",
    "}\n",
    "params['q'] = params['oq'] = '파이썬'\n",
    "headers = {\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
    "}\n",
    "resp = download(url, params, headers, 'GET')\n",
    "dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "urls = [{'url':_['href'], 'depth':1}\n",
    "        for _ in dom.select('div.r > a[href]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = list()\n",
    "\n",
    "while urls: # Queue\n",
    "    seed = urls.pop(-1)\n",
    "    seen.append(seed)\n",
    "    if seed['depth'] < 4: # 범위 제한 => Focused Crawling\n",
    "        dom = BeautifulSoup(download(seed['url']).text, 'html.parser')\n",
    "        for _ in [_['href'] for _ in dom.select('a[href]')\n",
    "                  if re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+',\n",
    "                              _['href'])]:\n",
    "            newUrls = urljoin(seed['url'], urlparse(_)[2])\n",
    "            if newUrls not in [_['url'] for _ in seen] \\\n",
    "            and newUrls not in [_['url'] for _ in urls]:\n",
    "                urls.append({'url':newUrls, 'depth':seed['depth']+1})\n",
    "        print(len(urls), len(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                  구글(검색어)-Queue(0)\n",
    "    [1]     [2]    3    4     5    6    7     8 -> done\n",
    "[1-1] 1-2 1-3 ..., 2-1, 2-2, ...\n",
    "1-1-1 1-1-2 ... => leaf\n",
    "=> BFS\n",
    "                  구글(검색어)-Stack(-1)\n",
    "    1     2    3    4     5    6    7     [8]\n",
    "                                      8-1 8-2 [8-3] ...,\n",
    "                                    8-3-1 [8-3-2] ... => leaf, done\n",
    "=> DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "구글(파이썬) -> 8개 검색결과(depth=1) -> 다시 링크타고 이동(depth=2) -> depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = '/www.domain.co.kr/service/1/2?key=value/'\n",
    "re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://search.naver.com/search.naver'\n",
    "params = {\n",
    "    'sm':'top_hty',\n",
    "    'fbm':0,\n",
    "    'ie':'utf8',\n",
    "    'query':''\n",
    "}\n",
    "params['query'] = '파이썬'\n",
    "resp = download(url, params, headers, 'GET')\n",
    "dom = BeautifulSoup(resp.content, 'html.parser')\n",
    "urls = [{'url':_['href'], 'depth':1}\n",
    "        for _ in dom.select('div.blog.section._blogBase._prs_blg dt > a[href]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = list()\n",
    "\n",
    "while urls: # Queue\n",
    "    seed = urls.pop(0)\n",
    "    seen.append(seed)\n",
    "    if seed['depth'] < 3: # 범위 제한 => Focused Crawling (수정)\n",
    "        dom = BeautifulSoup(download(seed['url']).text, 'html.parser')\n",
    "\n",
    "        for _ in [_['href'] if _.has_attr('href') else _['src'] # 여기\n",
    "                  for _ in dom.select('a[href], #mainFrame')\n",
    "                  if re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+',\n",
    "                _['href'] if _.has_attr('href') else _['src'])]: # 여기\n",
    "            newUrls = urljoin(seed['url'], _) # 여기\n",
    "            if urlparse(newUrls)[1] in ['blog.naver.com'] \\\n",
    "            and newUrls not in [_['url'] for _ in seen] \\\n",
    "            and newUrls not in [_['url'] for _ in urls]:# (수정)도메인 제한\n",
    "                urls.append({'url':newUrls, 'depth':seed['depth']+1})\n",
    "        print(len(urls), len(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('pagerank.db')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executescript('''\n",
    "    DROP TABLE IF EXISTS host;\n",
    "    CREATE TABLE host(\n",
    "        pk      INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "        netloc  TEXT NOT NULL\n",
    "    );\n",
    "    DROP TABLE IF EXISTS url;\n",
    "    CREATE TABLE url(\n",
    "        pk      INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "        fk      INTEGER NOT NULL,\n",
    "        ref     INTEGER NOT NULL,\n",
    "        path    TEXT NOT NULL,\n",
    "        crawled TEXT NOT NULL\n",
    "    );\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.google.com/search'\n",
    "params = {\n",
    "    'q':'',\n",
    "    'oq':'',\n",
    "    'aqs':'chrome..69i57j69i59j69i65l3j69i61j69i60j69i61.1205j0j7',\n",
    "    'sourceid':'chrome',\n",
    "    'ie':'UTF-8'\n",
    "}\n",
    "params['q'] = params['oq'] = '파이썬'\n",
    "headers = {\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
    "}\n",
    "resp = download(url, params, headers, 'GET')\n",
    "dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "for _ in dom.select('div.r > a[href]'):\n",
    "    urlseg = urlparse(_['href'])\n",
    "    # url => http://netloc/path?query\n",
    "    # netloc = htttp://netloc\n",
    "    netloc = '://'.join(urlseg[:2])\n",
    "    # path = path?query\n",
    "    path = urlseg[2]+('?'+urlseg[4] if urlseg[4] else '')\n",
    "    # host테이블에 netloc이 있는지 확인\n",
    "    cur.execute('SELECT pk FROM host WHERE netloc=? ORDER BY pk ASC LIMIT 0,1',\n",
    "            ['://'.join(urlseg[:2]),])\n",
    "    # 없다면,\n",
    "    if not cur.fetchone():\n",
    "        # host테이블에 netloc 추가\n",
    "        cur.execute('INSERT INTO host(netloc) VALUES(?)',\n",
    "                    ['://'.join(urlseg[:2]),])\n",
    "        con.commit()\n",
    "    # url테이블에 path 추가 (단, host테이블 netloc의 pk를 fk로 가져옴)\n",
    "    cur.execute('''\n",
    "    INSERT INTO url(fk, ref, path, crawled)\n",
    "    VALUES((SELECT pk FROM host WHERE netloc=? ORDER BY pk LIMIT 0, 1), ?, ?, 'N')\n",
    "    ''', [netloc, 0, path])\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while range(1000):#True:\n",
    "for _ in range(100):\n",
    "    cur.execute('''\n",
    "        SELECT host.pk, url.pk, host.netloc, url.path FROM url\n",
    "        INNER JOIN host ON host.pk=url.fk\n",
    "        WHERE url.crawled='N'\n",
    "        ORDER BY url.pk ASC\n",
    "        LIMIT 0,1\n",
    "    ''')\n",
    "    seed = cur.fetchone()\n",
    "\n",
    "    if not seed:\n",
    "        break\n",
    "        \n",
    "    cur.execute('''\n",
    "        UPDATE url SET crawled='Y' WHERE pk=?\n",
    "    ''', [seed[1],])\n",
    "    url = ''.join(seed[2:])\n",
    "    dom = BeautifulSoup(download(url).text, 'html.parser')\n",
    "    for _ in [_['href'] for _ in dom.select('a[href]')\n",
    "              if re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+',\n",
    "                          _['href'])]:\n",
    "        urlseg = urlparse(urljoin(url, _))\n",
    "        netloc = '://'.join(urlseg[:2])\n",
    "        path = urlseg[2]+('?'+urlseg[4] if urlseg[4] else '')\n",
    "        cur.execute('''\n",
    "            SELECT pk FROM host WHERE netloc=?\n",
    "            ORDER BY pk ASC LIMIT 0,1''', ['://'.join(urlseg[:2]),])\n",
    "        if not cur.fetchone():\n",
    "            cur.execute('INSERT INTO host(netloc) VALUES(?)',\n",
    "                        ['://'.join(urlseg[:2]),])\n",
    "            con.commit()\n",
    "            \n",
    "        cur.execute('''\n",
    "            SELECT count(pk) FROM url WHERE fk=? and path=?\n",
    "        ''', [seed[0], path])\n",
    "        \n",
    "        if cur.fetchone()[0] < 1:\n",
    "            cur.execute('''\n",
    "            INSERT INTO url(fk, ref, path, crawled)\n",
    "            VALUES((SELECT pk FROM host WHERE netloc=? ORDER BY pk LIMIT 0, 1), ?, ?, 'N')\n",
    "            ''', [netloc, seed[0], path])\n",
    "            con.commit()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT COUNT(pk) FROM host')\n",
    "cnt = cur.fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((cnt+1,cnt+1))\n",
    "v = np.zeros((cnt+1,))\n",
    "S = np.zeros((cnt+1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,cnt+1):\n",
    "    cur.execute('''\n",
    "        SELECT fk, count(fk) FROM url\n",
    "        WHERE ref=? GROUP BY fk''', [j,])\n",
    "    \n",
    "    for i in cur:\n",
    "        A[i[0],j] = i[1]\n",
    "        S[j] += i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA = A/S\n",
    "AA[np.isnan(AA)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[:] = 1/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    v = AA.dot(v)\n",
    "    print('Iter:{}'.format(_+1))\n",
    "    for _ in v.argsort()[::-1][:5]:\n",
    "        cur.execute('SELECT * FROM host WHERE pk=?', [str(_),])\n",
    "        print(cur.fetchone(), np.round(v[_], 10),\n",
    "              A[:,_].sum(), A[_].sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0])/np.array([0]), \\\n",
    "np.array([0])/np.array([1]), \\\n",
    "np.array([1])/np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.ppomppu.co.kr/zboard/zboard.php'\n",
    "\n",
    "params = {\n",
    "    'id':'ppomppu',\n",
    "    'page':1\n",
    "}\n",
    "headers = {\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling[링크] + Scraping[내용]\n",
    "# Focused Crawling - 게시판의 파라미터로 제한\n",
    "for page in range(1,10):\n",
    "    params['page'] = page\n",
    "    resp = download(url, params, headers, 'GET')\n",
    "    dom = BeautifulSoup(resp.text, 'lxml')\n",
    "    \n",
    "    # Scraping\n",
    "    for _ in dom.select('#revolution_main_table tr[class^=list]')[1:]:\n",
    "        item = dict()\n",
    "        td = _.find_all('td', recursive=False)\n",
    "        temp = title.findall(td[3].text)\n",
    "        frag1 = temp[0] if len(temp) > 0 else td[3].text.strip()\n",
    "        frag2 = td[-2].text.split('-')\n",
    "\n",
    "        item['mall'] = frag1[0].strip()\n",
    "        item['title'] = frag1[1].strip()\n",
    "        item['price'] = frag1[2].strip()\n",
    "        item['tu'] = frag2[0].strip() if len(frag2) > 1 else 0\n",
    "        item['td'] = frag2[1].strip() if len(frag2) > 1 else 0\n",
    "        print(item)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu'\n",
    "headers = {\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
    "}\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "while urls:\n",
    "    seed = urls.pop(0)\n",
    "    seen.append(seed)\n",
    "\n",
    "    dom = BeautifulSoup(download(seed).text, 'lxml')\n",
    "    for _ in [_['href']\n",
    "              for _ in dom.select('''\n",
    "              tr.list0 a[href], tr.list1 a[href], #page_list a[href]\n",
    "              ''')\n",
    "              if re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+', _['href'])]:\n",
    "        newUrls = urljoin(seed, _)\n",
    "        if newUrls not in urls \\\n",
    "        and newUrls not in seen:\n",
    "            urls.append(newUrls)\n",
    "    print(len(urls), len(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping\n",
    "for _ in dom.select('#revolution_main_table tr[class^=list]')[1:]:\n",
    "    item = dict()\n",
    "    td = _.find_all('td')\n",
    "    frag1 = title.findall(td[3].text)[0]\n",
    "    frag2 = td[-2].text.split('-')\n",
    "    \n",
    "    item['mall'] = frag1[0].strip()\n",
    "    item['title'] = frag1[1].strip()\n",
    "    item['price'] = frag1[2].strip()\n",
    "    item['tu'] = frag2[0].strip() if len(frag2) > 1 else 0\n",
    "    item['td'] = frag2[1].strip() if len(frag2) > 1 else 0\n",
    "    print(item)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = re.compile(r'[[]?(\\w+)[]]?([^(]+)[(](.+?)[)]')\n",
    "price = re.compile(r'(.+?)[ /](.+?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[]? => (문자)[ 있고, 없고  \\[\n",
    " (\\w+) => 글자 1개 이상 (1)\n",
    " []]? => (문자)] 있고, 없고\n",
    "\n",
    "([^(]+) => (그룹) (문자)(가 아닌 다른 문자가 1개 이상 (2)\n",
    "  [(] => (문자)( 있고\n",
    "    (.+?) => 아무글자나 1개 이상(Lazy)  (3)\n",
    "    [)] => (문자)) 있고\n",
    "    \n",
    "[(1=???)] (2=???????) ((3=????))\n",
    "[LG] 통돌이 모델명 (가격)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.ppomppu.co.kr/zboard/zboard.php'\n",
    "\n",
    "params = {\n",
    "    'id':'freeboard',\n",
    "    'page':1\n",
    "}\n",
    "headers = {\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append({'url':url, 'depth':1})\n",
    "\n",
    "while urls:\n",
    "    seed = urls.pop(0)\n",
    "    \n",
    "    if seed['depth'] > 2:\n",
    "        break\n",
    "        \n",
    "    resp = download(seed['url'])\n",
    "    dom = BeautifulSoup(resp.text, 'lxml')\n",
    "    \n",
    "    seen.append(seed)\n",
    "    \n",
    "    for _ in dom.select('''\n",
    "        .list0 > td:nth-of-type(3) > a,\n",
    "        .list1 > td:nth-of-type(3) > a,\n",
    "        #page_list a'''):\n",
    "        if _['href'][0] in ['v', '/']:\n",
    "            newurl = urljoin(url, _['href'])\n",
    "            if newurl not in [_['url'] for _ in urls] and\\\n",
    "               newurl not in [_['url'] for _ in seen]:\n",
    "                urls.append({'url':newurl,\n",
    "                             'depth':seed['depth']+1})\n",
    "            # 여기까지가 크롤링\n",
    "            # 여기서부터가 스크래핑\n",
    "            if dom.select_one('.board-contents') != None:\n",
    "                print(dom.select_one('.board-contents').name)\n",
    "    \n",
    "# resp = download(url, params, headers)\n",
    "# dom = BeautifulSoup(resp.text, 'lxml')\n",
    "# for _ in dom.select('#page_list a'):\n",
    "#     if _['href'].startswith('/'):\n",
    "#         print(urljoin(url, _['href']))\n",
    "#         resp = download(urljoin(url, _['href']))\n",
    "#         dom = BeautifulSoup(resp.text, 'lxml')\n",
    "#         if dom.select_one('.board-contents') != None:\n",
    "#             print(dom.select_one('.board-contents').name)\n",
    "#     print(_.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
